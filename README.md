# Simulación de ataques FDIA y detección en MMC

## Introducción

El objetivo de esta instancia es migrar el desarrollo de un proyecto de aprendizaje reforzado al lenguaje de programación Python, integrando las utilidades de los *framework* de aprendizaje reforzado, stable-baselines y gym, respectivamente. El proyecto de RL consiste en implementar la simulación de un sistema MMC (Convertidor Modular Multinivel), un dispositivo eléctrico con la capacidad de transportar energía a largas distancias (generalmente en el ámbito HVDC) y con características modulares que le permite al usuario aplicar un control de manera distribuida a cada una de las unidades que componen al sistema. Esta simulación presenta un entorno de aprendizaje por refuerzo, donde los agentes a controlar son un atacante generador de ciberataques tipo *False Data Injection* o FDIA, y un detector de dichos ataques.

Los métodos de entrenamiento a utilizar son, principalmente, TD3 para el atacante y DQN para el detector. Ambos métodos de entrenamiento corresponden a la categoría de *off-policy*, es decir, recurren a la estimación del valor esperado de la recompensa para tomar una acción. En el caso del atacante, se decidió añadir dos métodos de entrenamiento: SAC y PPO. 

## Modo de uso 
Para poder correr los entrenamientos, se debe tener en cuenta la instalación previa de las librerías anteriormente mencionadas, además de paquetes como Numpy, Matplotlib y Pytorch, los cuales sirvieron como apoyo adicional en las implementaciones de los ambientes y la visualización de los resultados. Los archivos .zip de la carpeta "agent-files" contienen los modelos pre-entrenados para atacante y detector, los cuales pueden probarse comentando con "\#" las definiciones de los modelos en los scripts de Python que tienen los sufijos "\_atk" o "\_detector". Los ambientes se encuentran en los archivos "env\_gym" y "detector", respectivamente, a cada uno de ellos les corresponden los archivos de simulación correspondientes (disponibles en "sim-files"). Tomar en consideración el copiar la ruta de la carpeta donde están los archivos de simulación, pues este es uno de los argumentos para definir cada ambiente.

Los hiperparámetros importantes que se pueden modificar en el entrenamiento de TD3 son, el ruido de exploración de política, el valor de $\tau$, el tamaño del perceptrón multicapa (diccionario disponible en "policy\_kwargs"), el *learning rate* y el tamaño del lote de entrenamiento de la red. Por otro lado, los hiperparámetros modificables en el entrenamiento DQN son los relacionados a la política $\varepsilon$*-greedy* (valor inicial y final de $\varepsilon$ y su tasa de cambio).
